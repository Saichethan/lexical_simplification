{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_test = pd.read_csv('./english/News_Test.tsv', sep='\\t', names = ['id','sentence','start','end','word','total_n','total_none_n','count_n','count_nn','bin','prob'])\n",
    "news_train = pd.read_csv('./english/News_Train.tsv', sep='\\t', names = ['id','sentence','start','end','word','total_n','total_none_n','count_n','count_nn','bin','prob'])\n",
    "news_dev = pd.read_csv('./english/News_Dev.tsv', sep='\\t', names = ['id','sentence','start','end','word','total_n','total_none_n','count_n','count_nn','bin','prob'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikinews_test = pd.read_csv('./english/WikiNews_Test.tsv', sep='\\t', names = ['id','sentence','start','end','word','total_n','total_none_n','count_n','count_nn','bin','prob'])\n",
    "wikinews_train = pd.read_csv('./english/WikiNews_Train.tsv', sep='\\t', names = ['id','sentence','start','end','word','total_n','total_none_n','count_n','count_nn','bin','prob'])\n",
    "wikinews_dev = pd.read_csv('./english/WikiNews_Dev.tsv', sep='\\t', names = ['id','sentence','start','end','word','total_n','total_none_n','count_n','count_nn','bin','prob'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_test = pd.read_csv('./english/Wikipedia_Test.tsv', sep='\\t', names = ['id','sentence','start','end','word','total_n','total_none_n','count_n','count_nn','bin','prob'])\n",
    "wikipedia_train = pd.read_csv('./english/Wikipedia_Train.tsv', sep='\\t', names = ['id','sentence','start','end','word','total_n','total_none_n','count_n','count_nn','bin','prob'])\n",
    "wikipedia_dev = pd.read_csv('./english/Wikipedia_Dev.tsv', sep='\\t', names = ['id','sentence','start','end','word','total_n','total_none_n','count_n','count_nn','bin','prob'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [wikipedia_train,wikipedia_test,wikipedia_dev,news_test,news_train,news_dev,\n",
    "         wikinews_test,wikinews_train,wikinews_dev]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "total['is_word'] = total.word.apply(lambda x: len(x.split(' '))==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = total[total['is_word']==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = total[total['count_n']>4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = total.sort_values(by = 'count_n', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "#doc = nlp(u'Apple is looking at buying U.K. startup for $1 billion')\n",
    "\n",
    "#for token in doc:\n",
    " #   print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "  #          token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "total['spacy'] = total.sentence.apply(lambda x: nlp(u''+x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "total['tokenized'] = total.spacy.apply(lambda x: [str(token) for token in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "total['pos'] = total.spacy.apply(lambda x: [(token,token.pos_) for token in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_pos(row):\n",
    "    word = row['word']\n",
    "    pos_tags = row['pos']\n",
    "    \n",
    "    for word_,pos in pos_tags:\n",
    "        if word == str(word_):\n",
    "            return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "def get_word_index(row):\n",
    "    word = row['word']\n",
    "    sentence = [str(token) for token in row['spacy']]\n",
    "    try:\n",
    "        index = sentence.index(word)\n",
    "        return index\n",
    "    except:\n",
    "        return 'None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "total['pos_word'] = total.apply(get_word_pos,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "total['word_index'] = total.apply(get_word_index,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = total[total['word_index']!='None']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "total['sentence_length'] = total.sentence.apply(lambda x: len(x))\n",
    "total['word_length'] = total.word.apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns = total[total['pos_word']=='NOUN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbs = total[total['pos_word']=='VERB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = total[total['pos_word']=='ADJ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "sns.distplot(nouns.word_length.values)\n",
    "sns.set(color_codes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/allennlp/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "from class_test import Complex\n",
    "from class_test import Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def individual_simplification(tokenized,index):\n",
    "    #tokenized = row['tokenized']\n",
    "    #index = row['word_index']\n",
    "    sentence_object = Complex(tokenized,0.5)\n",
    "    word_object = Word(sentence_object, index)\n",
    "    #get the synonyms for this word\n",
    "    word_object.get_synonyms()\n",
    "\n",
    "    #rank these synonyms\n",
    "\n",
    "    synonym_dataframe = word_object.get_synonym_dataframe()\n",
    "    sub = synonym_dataframe.synonyms.head(1)\n",
    "    \n",
    "    if synonym != []:\n",
    "        sentence_object.make_simplification(synonym, word_object.index)\n",
    "        new_sentence = sentence_object.tokenized\n",
    "\n",
    "    return sub,new_sentence,synonym_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "dataframes = []\n",
    "substitutes = []\n",
    "replacement_sentences = []\n",
    "\n",
    "index_list = nouns.word_index.values\n",
    "tokenized_list = nouns.tokenized.values\n",
    "\n",
    "for i,(tokenized,index) in enumerate(zip(tokenized_list,index_list)):\n",
    "    print(i)\n",
    "    try:\n",
    "        substitute, tok_sentence, syn_dataframe = individual_simplification(tokenized,index)\n",
    "        dataframes.append(syn_dataframe)\n",
    "        substitutes.append(substitute)\n",
    "        replacement_sentences.append(tok_sentence)\n",
    "    except Exception as e: \n",
    "        print(e)\n",
    "        dataframes.append(pd.DataFrame())\n",
    "        substitutes.append('None')\n",
    "        replacement_sentences.append(['None'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adj_ = adj[['sentence','sentence_length','word','word_index','word_length','prob']]\n",
    "#nouns_ = nouns[['sentence','sentence_length','word','word_index','word_length','prob']]\n",
    "#verbs_ = verbs[['sentence','sentence_length','word','word_index','word_length','prob']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with pd.ExcelWriter('cw_dataset.xlsx') as writer:  # doctest: +SKIP\n",
    " #       nouns_.to_excel(writer, sheet_name='nouns')\n",
    "  #      verbs_.to_excel(writer, sheet_name='verbs')\n",
    "   #     adj_.to_excel(writer, sheet_name='adj')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
